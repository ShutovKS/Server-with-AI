#################################
# LLM CONFIGURATION
#################################

# Основная LLM-модель
LLM_MODEL=qwen2.5-7b-instruct

# Максимальное количество новых токенов при генерации ответа
LLM_MAX_NEW_TOKENS=512

# Провайдер LLM (lm-studio, huggingface)
LLM_PROVIDER=lm-studio

# Базовый URL, если используем lm-studio (или другой локальный сервис)
LLM_BASE_URL=http://localhost:1234/v1

# Локальная модель или внешняя (True/False)
LOCAL_LLM_USE=True

# Модель должна давать многословные ответы (True/False)
VERBOSE=False

#################################
# EMBEDDING CONFIGURATION
#################################

# Модель эмбеддингов
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# Размерность эмбеддингов
EMBEDDING_DIM=2024
# Бэкенд (PyTorch, TensorFlow и т.п.)
EMBEDDING_BACKEND=torch


#################################
# CHUNK SETTINGS
#################################

# Максимальный размер одного «куска» текста
CHUNK_SIZE=2048
# Насколько сильно куски должны перекрываться
CHUNK_OVERLAP=256


#################################
# STORAGE / CACHE
#################################

# Директория для кеша
STORAGE_CACHE_DIR=.cache
# Путь для хранилища файлов
STORAGE_DIR=storage
# Путь для хранилища Chroma
CHROMA_PATH=storage
# Данные (исходные файлы, документы и т.п.)
DATA_DIR=data

# Настройки телеметрии
ANONYMIZED_TELEMETRY=False


#################################
# APPLICATION SETTINGS
#################################

# Хост и порт для веб-приложения
APP_HOST=localhost
APP_PORT=8000


#################################
# SYSTEM PROMPT
#################################

SYSTEM_PROMPT="Вы — полезный помощник, помогающий пользователям с их вопросами.
У вас есть доступ к базе знаний, включающей факты, с которых вам следует начать, чтобы найти ответ на вопрос пользователя.
Используйте инструмент query engine для извлечения фактов из базы знаний.
Конечный ответ пользователю должен быть на русском языке."
